{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice: Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This practice notebook is based on Evgeny Sokolov's <a href=\"https://github.com/esokolov/ml-course-hse/blob/master/2017-fall/seminars/sem07-trees.ipynb\">materials</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T10:10:49.596735Z",
     "start_time": "2021-03-22T10:10:48.602322Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's generate the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T10:10:49.603300Z",
     "start_time": "2021-03-22T10:10:49.598198Z"
    }
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "\n",
    "X = np.sort(5 * rng.rand(80, 1), axis=0)\n",
    "y = np.sin(X).ravel()  # plane sinus function\n",
    "y[::5] += 3 * (0.5 - rng.rand(16))  # add random outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then train our basic models: Decision Tree regressors with different depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T10:10:49.667221Z",
     "start_time": "2021-03-22T10:10:49.658149Z"
    }
   },
   "outputs": [],
   "source": [
    "regr_tree_2 = DecisionTreeRegressor(max_depth=2)\n",
    "regr_tree_2.fit(X, y)\n",
    "\n",
    "regr_tree_5 = DecisionTreeRegressor(max_depth=5)\n",
    "regr_tree_5.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our models, we are able to retrieve predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T10:10:50.365285Z",
     "start_time": "2021-03-22T10:10:50.362509Z"
    }
   },
   "outputs": [],
   "source": [
    "grid = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n",
    "pred_surface_2 = regr_tree_2.predict(grid)\n",
    "pred_surface_5 = regr_tree_5.predict(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the results. Does the model fit data well? Which parameter is better?\n",
    "\n",
    "How does maximum depth influence the solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T10:10:51.144415Z",
     "start_time": "2021-03-22T10:10:50.906059Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "plt.scatter(X, y, s=50, color=\"black\", label=\"data\")\n",
    "plt.plot(grid, pred_surface_2, color=\"green\", label=\"max_depth=2\", linewidth=3)\n",
    "plt.plot(grid, pred_surface_5, color=\"red\", label=\"max_depth=5\", linewidth=3)\n",
    "\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"target\")\n",
    "plt.title(\"Decision Tree Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T10:10:51.334301Z",
     "start_time": "2021-03-22T10:10:51.330586Z"
    }
   },
   "outputs": [],
   "source": [
    "np.unique(pred_surface_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T10:11:03.120726Z",
     "start_time": "2021-03-22T10:11:03.117871Z"
    }
   },
   "outputs": [],
   "source": [
    "np.unique(pred_surface_5).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Informativity criteria\n",
    "\n",
    "Let\n",
    "* $R_m$ - a set of all training data objects that refer to the node $m$, \n",
    "* $N_m=|R_m|$.\n",
    "* $p_{mk}$ - a fraction of objects of each class $k\\in\\{1, ..., K\\}$, that got to the node $m$: $p_{mk}=\\frac{1}{N_m} \\sum\\limits_{x_i\\in R_m} [y_i = k]$.\n",
    "* $k_m = arg \\max\\limits_{k} p_{mk}$ - the majority class in the node $m$.\n",
    "\n",
    "**The corresponding informativity criterion:**\n",
    "$$\n",
    "Q(R_m, j, s) = H(R_m) - \\frac{N_l}{N_m} H(R_l) - \\frac{N_r}{N_m} H(R_r).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jini index\n",
    "\n",
    "$$ H(R_m) = \\sum\\limits_{k \\neq k'}p_{mk}p_{mk'}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shannon's entropy criterion\n",
    "Given a discrete random variable,\n",
    "that can take $K$ values with probabilities $p_1, \\dots, p_K$\n",
    "respectively.\n",
    "\n",
    "***Entropy*** of this random vairable is defined as follows:\n",
    "$$H(p) = -\\sum_{k = 1}^{K} p_k \\log_2 p_k$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T22:55:30.771906Z",
     "start_time": "2021-03-21T22:55:30.543967Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "grid = np.linspace(0, 1, 100)\n",
    "gini = 2 * grid * (1 - grid)\n",
    "missclass = 1 - np.maximum(grid, 1 - grid)  # np.maximum - elementwise function unlike np.max\n",
    "eps = 1e-10  # for numerical stability\n",
    "entropy = -grid * np.log2(grid + eps) - (1 - grid) * np.log2(1 - grid + eps)\n",
    "\n",
    "plt.plot(grid, gini, label=\"gini\")\n",
    "plt.plot(grid, 2 * gini, label=\"2 * gini\")\n",
    "plt.plot(grid, missclass, label=\"missclass\")\n",
    "plt.plot(grid, 2 * missclass, label=\"2 * missclass\")\n",
    "plt.plot(grid, entropy, label=\"entropy\")\n",
    "\n",
    "plt.xlabel(\"p+\")\n",
    "plt.ylabel(\"criterion\")\n",
    "plt.title(\"Quality criterion as a function of p+ (binary case)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression criteria\n",
    "In regression tasks the default choice of stopping criteria is the variance of objects in the leaf:\n",
    "$$\n",
    "H_R(R_m) = \\frac{1}{N_m} \\sum_{(x_i,\\,y_i) \\in R_m} \\left(y_i-\\frac{1}{N_m}\\sum_{(x_i,\\,y_i) \\in R_m} y_j \\right)^2.\n",
    "$$\n",
    "\n",
    "However, different criteria are possible - for example, Mean Absolute Deviation from the median."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopping criteria\n",
    "\n",
    "For any training set it is possible to construct a decision tree perfectly fitting this sample. That is, the training error can be minimized to zero.\n",
    "\n",
    "If we consider objects as points in the feature space, each of those points (training objects) can be surrounded by a n-dimensional cube, not containing any other points. That means, we can construct a decision tree, in which all the leaves are such cubes. There will be $n$ leaves ($n$ - the size of training data), and the error will be 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, such an approach will lead to **severe overfitting**. In general, decision trees can fit very complicated dependencies and are prone to overfitting.\n",
    "\n",
    "That leads to the question - how can we determine, if we should stop splitting the data and call the current node a leaf?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a toy regression task.\n",
    "Objects will be points on 2-dim surface, and a target label will be the distance between the point and the center of coordinates (0, 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T22:55:30.776884Z",
     "start_time": "2021-03-21T22:55:30.773143Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_grid(data):\n",
    "    x_min, x_max = data[:, 0].min() - 1, data[:, 0].max() + 1\n",
    "    y_min, y_max = data[:, 1].min() - 1, data[:, 1].max() + 1\n",
    "    return np.meshgrid(\n",
    "        np.arange(x_min, x_max, 0.01),\n",
    "        np.arange(y_min, y_max, 0.01),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we generate the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T22:55:30.783054Z",
     "start_time": "2021-03-21T22:55:30.780441Z"
    }
   },
   "outputs": [],
   "source": [
    "data_x = np.random.normal(size=(100, 2))\n",
    "data_y = (data_x[:, 0] ** 2 + data_x[:, 1] ** 2) ** 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T22:55:30.909956Z",
     "start_time": "2021-03-21T22:55:30.785287Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(data_x[:, 0], data_x[:, 1], c=data_y, s=100, cmap=\"spring\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T22:55:30.916049Z",
     "start_time": "2021-03-21T22:55:30.911756Z"
    }
   },
   "outputs": [],
   "source": [
    "clf = DecisionTreeRegressor()\n",
    "clf.fit(data_x, data_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T22:55:31.313281Z",
     "start_time": "2021-03-21T22:55:30.917333Z"
    }
   },
   "outputs": [],
   "source": [
    "xx, yy = get_grid(data_x)\n",
    "\n",
    "predicted = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pcolormesh(xx, yy, predicted, cmap=\"spring\", shading=\"auto\")\n",
    "plt.scatter(data_x[:, 0], data_x[:, 1], c=data_y, s=100, cmap=\"spring\", edgecolor=\"k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how the decision surface will look like depending on:\n",
    "- minimal num. of objects inside a leaf\n",
    "- maximal depth of the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T22:55:34.057417Z",
     "start_time": "2021-03-21T22:55:31.315231Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 14))\n",
    "\n",
    "for i, max_depth in enumerate([2, 4, None]):\n",
    "    for j, min_samples_leaf in enumerate([15, 5, 1]):\n",
    "\n",
    "        clf = DecisionTreeRegressor(max_depth=max_depth, min_samples_leaf=min_samples_leaf)\n",
    "        clf.fit(data_x, data_y)\n",
    "\n",
    "        xx, yy = get_grid(data_x)\n",
    "        predicted = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "        plt.subplot2grid((3, 3), (i, j))\n",
    "        plt.pcolormesh(xx, yy, predicted, cmap=\"spring\", shading=\"auto\")\n",
    "        plt.scatter(data_x[:, 0], data_x[:, 1], c=data_y, s=30, cmap=\"spring\", edgecolor=\"k\")\n",
    "        plt.title(\"max_depth=\" + str(max_depth) + \", min_samples_leaf: \" + str(min_samples_leaf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "- We can clearly see that large values of max. depth and low values of min. number of leaf objects lead to near-perfect training data scores and overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instability of decision trees\n",
    "\n",
    "Decision trees are not very stable: if we slighly change the training set, the resulting classifier may change radically.\n",
    "Let's see an example of this property. We are going to train the tree on different 90% subsamples from the training data and look at the tree structure for those subsets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T22:55:35.070144Z",
     "start_time": "2021-03-21T22:55:34.058740Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "\n",
    "for i in range(3):\n",
    "    clf = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "    indecies = np.random.randint(data_x.shape[0], size=int(data_x.shape[0] * 0.9))\n",
    "    clf.fit(data_x[indecies], data_y[indecies])\n",
    "\n",
    "    xx, yy = get_grid(data_x)\n",
    "    predicted = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "    plt.subplot2grid((1, 3), (0, i))\n",
    "    plt.pcolormesh(xx, yy, predicted, cmap=\"winter\", shading=\"auto\")\n",
    "    plt.scatter(data_x[:, 0], data_x[:, 1], c=data_y, s=30, cmap=\"winter\", edgecolor=\"k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with categorical features \n",
    "\n",
    "There are several approaches to work with categorical features in decision trees:\n",
    "\n",
    "* One-hot encoding, Mean target encoding, binary encoding\n",
    "* In some frameworks (CatBoost), the number of outgoing edges in the node can differ from 2. In case of a categorical split, we may create n outgoing edges (where n is the number of categories). This will inherently take into account each category.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pros and cons of Decision Trees:\n",
    "\n",
    "**Pros**\n",
    " * Interpretability\n",
    " * Easy to use for both regression and classification\n",
    " * Can work with heterogenuous data\n",
    " \n",
    "**Cons**\n",
    " * If the set is linearly divisible, linear models perform much better\n",
    " * Prone to overfitting\n",
    " * Instability to noise, training set change, criterion change etc.\n",
    " \n",
    "**Possible ways to improve the algorithm**\n",
    " * Pruning\n",
    " * Compositions of multiple trees (bagging, Random Forest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
