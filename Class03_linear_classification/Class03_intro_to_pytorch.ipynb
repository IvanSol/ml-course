{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# week0_03: Intro to PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*special thanks to YSDA team for provided materials*\n",
    "\n",
    "What comes today:\n",
    "- Introduction to PyTorch\n",
    "- Automatic gradient computation\n",
    "- Logistic regression (it's a neural network, actually ;) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logo](https://pytorch.org/tutorials/_static/pytorch-logo-dark.svg)\n",
    "\n",
    "__This notebook__ will teach you to use pytorch low-level core. You can read about it [here](http://pytorch.org/).\n",
    "\n",
    "__Pytorch feels__ differently than other frameworks (like tensorflow/theano) on almost every level. TensorFlow makes your code live in two \"worlds\" simultaneously:  symbolic graphs and actual tensors. First you declare a symbolic \"recipe\" of how to get from inputs to outputs, then feed it with actual minibatches of data.  In pytorch, __there's only one world__: all tensors have a numeric value.\n",
    "\n",
    "You compute outputs on the fly without pre-declaring anything. The code looks exactly as in pure numpy with one exception: pytorch computes gradients for you. And can run stuff on GPU. And has a number of pre-implemented building blocks for your neural nets. [And a few more things.](https://medium.com/towards-data-science/pytorch-vs-tensorflow-spotting-the-difference-25c75777377b)\n",
    "\n",
    "Let's dive into it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:06:13.601304Z",
     "start_time": "2021-03-20T11:06:13.599422Z"
    }
   },
   "outputs": [],
   "source": [
    "# If you are using colab, uncomment this cell\n",
    "\n",
    "# !wget https://raw.githubusercontent.com/girafe-ai/ml-course/22f_basic/week0_03_linear_classification/notmnist.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:06:14.736527Z",
     "start_time": "2021-03-20T11:06:14.038670Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.functional as F\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:06:14.742256Z",
     "start_time": "2021-03-20T11:06:14.737944Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:06:15.250588Z",
     "start_time": "2021-03-20T11:06:15.246484Z"
    }
   },
   "outputs": [],
   "source": [
    "# numpy world\n",
    "\n",
    "x = np.arange(16).reshape(4, 4)\n",
    "\n",
    "print(\"X :\\n%s\\n\" % x)\n",
    "print(\"X.shape : %s\\n\" % (x.shape,))\n",
    "print(\"add 5 :\\n%s\\n\" % (x + 5))\n",
    "print(\"X*X^T  :\\n%s\\n\" % np.dot(x, x.T))\n",
    "print(\"mean over cols :\\n%s\\n\" % (x.mean(axis=-1)))\n",
    "print(\"cumsum of cols :\\n%s\\n\" % (np.cumsum(x, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:06:15.848120Z",
     "start_time": "2021-03-20T11:06:15.838967Z"
    }
   },
   "outputs": [],
   "source": [
    "# pytorch world\n",
    "\n",
    "x = np.arange(16).reshape(4, 4)\n",
    "\n",
    "x = torch.tensor(x, dtype=torch.float32)  # or torch.arange(0,16).view(4,4)\n",
    "\n",
    "print(\"X :\\n%s\" % x)\n",
    "print(\"X.shape : %s\\n\" % (x.shape,))\n",
    "print(\"add 5 :\\n%s\\n\" % (x + 5))\n",
    "print(\"X*X^T  :\\n%s\\n\" % torch.matmul(x, x.transpose(1, 0)))  # short: x.mm(x.t())\n",
    "print(\"mean over cols :\\n%s\\n\" % torch.mean(x, dim=-1))\n",
    "print(\"cumsum of cols :\\n%s\" % torch.cumsum(x, dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### NumPy and Pytorch\n",
    "\n",
    "As you can notice, pytorch allows you to hack stuff much the same way you did with numpy. This means that you can _see the numeric value of any tensor at any moment of time_. Debugging such code can be done with by printing tensors or using any debug tool you want (e.g. [gdb](https://wiki.python.org/moin/DebuggingWithGdb)).\n",
    "\n",
    "You could also notice the a few new method names and a different API. So no, there's no compatibility with numpy [yet](https://github.com/pytorch/pytorch/issues/2228) and yes, you'll have to memorize all the names again. Get excited!\n",
    "\n",
    "![img](http://i0.kym-cdn.com/entries/icons/original/000/017/886/download.jpg)\n",
    "\n",
    "For example, \n",
    "* If something takes a list/tuple of axes in numpy, you can expect it to take *args in pytorch\n",
    " * `x.reshape([1,2,8]) -> x.view(1,2,8)`\n",
    "* You should swap _axis_ for _dim_ in operations like mean or cumsum\n",
    " * `x.sum(axis=-1) -> x.sum(dim=-1)`\n",
    "* most mathematical operations are the same, but types an shaping is different\n",
    " * `x.astype('int64') -> x.type(torch.LongTensor)`\n",
    "\n",
    "To help you acclimatize, there's a [table](https://github.com/torch/torch7/wiki/Torch-for-Numpy-users) covering most new things. There's also a neat [documentation page](http://pytorch.org/docs/master/).\n",
    "\n",
    "Finally, if you're stuck with a technical problem, we recommend searching [pytorch forumns](https://discuss.pytorch.org/). Or just googling, which usually works just as efficiently. \n",
    "\n",
    "If you feel like you almost give up, remember two things: __GPU__ and __free gradients__. Besides you can always jump back to numpy with x.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warmup: trigonometric knotwork\n",
    "\n",
    "_inspired by [this post](https://www.quora.com/What-are-the-most-interesting-equation-plots)_\n",
    "\n",
    "There are some simple mathematical functions with cool plots. For one, consider this:\n",
    "\n",
    "$$ x(t) = t - 1.5 * cos( 15 t) $$\n",
    "$$ y(t) = t - 1.5 * sin( 16 t) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:06:18.125078Z",
     "start_time": "2021-03-20T11:06:17.951606Z"
    }
   },
   "outputs": [],
   "source": [
    "t = torch.linspace(-10, 10, steps=10000)\n",
    "\n",
    "# compute x(t) and y(t) as defined above\n",
    "x = # YOUR CODE HERE\n",
    "y = # YOUR CODE HERE\n",
    "\n",
    "plt.plot(x.numpy(), y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you're done early, try adjusting the formula and seing how  it affects the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic gradients\n",
    "\n",
    "Any self-respecting DL framework must do your backprop for you. Torch handles this with the `autograd` module.\n",
    "\n",
    "The general pipeline looks like this:\n",
    "* When creating a tensor, you mark it as `requires_grad`:\n",
    "    * __```torch.zeros(5, requires_grad=True)```__\n",
    "    * torch.tensor(np.arange(5), dtype=torch.float32, requires_grad=True)\n",
    "* Define some differentiable `loss = arbitrary_function(a)`\n",
    "* Call `loss.backward()`\n",
    "* Gradients are now available as ```a.grads```\n",
    "\n",
    "__Here's an example:__ let's fit a linear regression on Boston house prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:06:26.893833Z",
     "start_time": "2021-03-20T11:06:26.270591Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.datasets import load_boston\n",
    "except ImportError:\n",
    "    # This is a workaround to use Boston dataset with new versions of sklearn where it was removed:\n",
    "    print('Your sklearn version seems to be too new and does not contain boston dataset. Dataset will be downloaded from kaggle')\n",
    "    import kagglehub\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    # Download latest version\n",
    "    path = kagglehub.dataset_download(\"arunjangir245/boston-housing-dataset\")\n",
    "\n",
    "    class BostonDS:\n",
    "        def __init__(self, data):\n",
    "            self.data = data.iloc[:,:-1].values\n",
    "            self.target = data.iloc[:,-1].values\n",
    "        \n",
    "    load_boston = lambda: BostonDS(pd.read_csv(os.path.join(path, 'BostonHousing.csv')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:06:27.033389Z",
     "start_time": "2021-03-20T11:06:26.895470Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "plt.scatter(boston.data[:, -1], boston.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:06:27.393913Z",
     "start_time": "2021-03-20T11:06:27.391125Z"
    }
   },
   "outputs": [],
   "source": [
    "w = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "x = torch.tensor(boston.data[:, -1] / 10, dtype=torch.float32)\n",
    "y = torch.tensor(boston.target, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:06:28.025275Z",
     "start_time": "2021-03-20T11:06:28.022490Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = w * x + b\n",
    "loss = torch.mean((y_pred - y) ** 2)\n",
    "\n",
    "# propagete gradients\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradients are now stored in `.grad` of those variables that require them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:06:29.306517Z",
     "start_time": "2021-03-20T11:06:29.302708Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"dL/dw = {}\\n\".format(w.grad))\n",
    "print(\"dL/db = {}\\n\".format(b.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you compute gradient from multiple losses, the gradients will add up at variables, therefore it's useful to __zero the gradients__ between iteratons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:06:30.616231Z",
     "start_time": "2021-03-20T11:06:30.614325Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:06:33.981724Z",
     "start_time": "2021-03-20T11:06:31.582379Z"
    }
   },
   "outputs": [],
   "source": [
    "w = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "x = torch.tensor(boston.data[:, -1] / 10, dtype=torch.float32)\n",
    "y = torch.tensor(boston.target, dtype=torch.float32)\n",
    "\n",
    "for i in range(200):\n",
    "    y_pred = w * x + b\n",
    "    loss = torch.mean((y_pred - y) ** 2)\n",
    "    loss.backward()\n",
    "\n",
    "    w.data -= 0.05 * w.grad.data\n",
    "    b.data -= 0.05 * b.grad.data\n",
    "\n",
    "    # zero gradients\n",
    "    w.grad.data.zero_()\n",
    "    b.grad.data.zero_()\n",
    "\n",
    "    # the rest of code is just bells and whistles\n",
    "    if (i + 1) % 5 == 0:\n",
    "        clear_output(True)\n",
    "        plt.scatter(x.data.numpy(), y.data.numpy())\n",
    "        plt.plot(x.data.numpy(), y_pred.data.numpy(), color=\"orange\", linewidth=5)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"loss = \", loss.data.numpy())\n",
    "        if loss.data.numpy() < 0.5:\n",
    "            print(\"Done!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Quest__: try implementing and writing some nonlinear regression. You can try quadratic features or some trigonometry, or a simple neural network. The only difference is that now you have more variables and a more complicated `y_pred`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suprizingly, we were walking really close to the edge. Look a few cells above. We have divided the `x` values by 10 times. Let's see what happens if we don't:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:06:38.580152Z",
     "start_time": "2021-03-20T11:06:36.062190Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now we do not divide the x values by 10. See what happens next\n",
    "x = torch.tensor(boston.data[:, -1], dtype=torch.float32) # / 10\n",
    "\n",
    "w = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "grad_history = []\n",
    "\n",
    "for i in range(100):\n",
    "    y_pred = w * x + b\n",
    "    loss = torch.mean((y_pred - y) ** 2)\n",
    "    loss.backward()\n",
    "    grad_history.append((w.grad.item(), b.grad.item()))\n",
    "\n",
    "    # Be extremely careful with accessing the .data attribute!\n",
    "    w.data -= 0.05 * w.grad.data\n",
    "    b.data -= 0.05 * b.grad.data\n",
    "\n",
    "    # zero gradients\n",
    "    w.grad.data.zero_()\n",
    "    b.grad.data.zero_()\n",
    "\n",
    "    # the rest of code is just bells and whistles\n",
    "    if (i + 1) % 5 == 0:\n",
    "        clear_output(True)\n",
    "        plt.scatter(x.data.numpy(), y.data.numpy())\n",
    "        plt.scatter(x.data.numpy(), y_pred.data.numpy(), color=\"orange\", linewidth=5)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"loss = \", loss.data.numpy())\n",
    "        if loss.data.numpy() < 0.5:\n",
    "            print(\"Done!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:06:39.028899Z",
     "start_time": "2021-03-20T11:06:38.926878Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot([element[0] for element in grad_history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:06:44.665153Z",
     "start_time": "2021-03-20T11:06:44.662866Z"
    }
   },
   "outputs": [],
   "source": [
    "print(grad_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the gradients have exploded. We could avoid it by using the smaller _learning rate_ or by using _gradient clipping_.\n",
    "\n",
    "__You should be really careful using the gradient descent even with such simple models as linear or logistic regression.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember!**\n",
    "![img](https://media.giphy.com/media/3o751UMCYtSrRAFRFC/giphy.gif)\n",
    "\n",
    "When dealing with more complex stuff like neural network, it's best if you use tensors the way samurai uses his sword. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-level pytorch\n",
    "\n",
    "So far we've been dealing with low-level torch API. While it's absolutely vital for any custom losses or layers, building large neura nets in it is a bit clumsy.\n",
    "\n",
    "Luckily, there's also a high-level torch interface with a pre-defined layers, activations and training algorithms. \n",
    "\n",
    "We'll cover them as we go through a simple image recognition problem: classifying letters into __\"A\"__ vs __\"B\"__.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:06:48.559449Z",
     "start_time": "2021-03-20T11:06:48.534377Z"
    }
   },
   "outputs": [],
   "source": [
    "from notmnist import load_notmnist  # file notmnist.py at this directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:06:50.585973Z",
     "start_time": "2021-03-20T11:06:49.038171Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_notmnist(letters=\"AB\")\n",
    "X_train, X_test = X_train.reshape([-1, 784]), X_test.reshape([-1, 784])\n",
    "\n",
    "print(\"Train size = %i, test_size = %i\" % (len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:06:52.068828Z",
     "start_time": "2021-03-20T11:06:51.918518Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in [0, 1]:\n",
    "    plt.subplot(1, 2, i + 1)\n",
    "    plt.imshow(X_train[i].reshape([28, 28]))\n",
    "    plt.title(str(y_train[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with layers. The main abstraction here is __`torch.nn.Module`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:06:55.575937Z",
     "start_time": "2021-03-20T11:06:55.574207Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:06:56.137053Z",
     "start_time": "2021-03-20T11:06:56.134791Z"
    }
   },
   "outputs": [],
   "source": [
    "print(nn.Module.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a vast library of popular layers and architectures already built for ya'.\n",
    "\n",
    "This is a binary classification problem, so we'll train a __Logistic Regression with sigmoid__.\n",
    "$$P(y_i | X_i) = \\sigma(W \\cdot X_i + b) ={ 1 \\over {1+e^{- [W \\cdot X_i + b]}} }$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:06:58.040714Z",
     "start_time": "2021-03-20T11:06:58.037987Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a network that stacks layers on top of each other\n",
    "model = nn.Sequential()\n",
    "\n",
    "# add first \"dense\" layer with 784 input units and 1 output unit.\n",
    "model.add_module(\"l1\", nn.Linear(784, 1))\n",
    "\n",
    "# add softmax activation for probabilities. Normalize over axis 1\n",
    "# note: layer names must be unique\n",
    "model.add_module(\"l2\", nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:07:01.328670Z",
     "start_time": "2021-03-20T11:07:01.326305Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Weight shapes:\", [w.shape for w in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:07:01.842288Z",
     "start_time": "2021-03-20T11:07:01.837811Z"
    }
   },
   "outputs": [],
   "source": [
    "# create dummy data with 3 samples and 784 features\n",
    "x = torch.tensor(X_train[:3], dtype=torch.float32)\n",
    "y = torch.tensor(y_train[:3], dtype=torch.float32)\n",
    "\n",
    "# compute outputs given inputs, both are variables\n",
    "y_predicted = model(x)[:, 0]\n",
    "\n",
    "y_predicted  # display what we've got"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define a loss function for our model.\n",
    "\n",
    "The natural choice is to use binary crossentropy (aka logloss, negative llh):\n",
    "$$ L = {1 \\over N} \\underset{X_i,y_i} \\sum - [  y_i \\cdot log P(y_i | X_i) + (1-y_i) \\cdot log (1-P(y_i | X_i)) ]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:07:03.302524Z",
     "start_time": "2021-03-20T11:07:03.230065Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "crossentropy = # YOUR CODE HERE\n",
    "loss = # YOUR CODE HERE\n",
    "\n",
    "assert tuple(crossentropy.size()) == (3,), \"Crossentropy must be a vector with element per sample\"\n",
    "assert tuple(loss.size()) == (1,), \"Loss must be scalar. Did you forget the mean/sum?\"\n",
    "assert (\n",
    "    crossentropy.data.numpy()[0] > 0\n",
    "), \"Crossentropy must non-negative, zero only for perfect prediction\"\n",
    "assert loss.data.numpy()[0] <= np.log(\n",
    "    5\n",
    "), \"Loss is too large even for untrained model. Please double-check it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ you can also find many such functions in `torch.nn.functional`, just type __`F.<tab>`__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Torch optimizers__\n",
    "\n",
    "When we trained Linear Regression above, we had to manually .zero_() gradients on both our variables. Imagine that code for a 50-layer network.\n",
    "\n",
    "Again, to keep it from getting dirty, there's `torch.optim` module with pre-implemented algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:07:10.478800Z",
     "start_time": "2021-03-20T11:07:10.471190Z"
    }
   },
   "outputs": [],
   "source": [
    "opt = torch.optim.RMSprop(model.parameters(), lr=0.01)\n",
    "\n",
    "# here's how it's used:\n",
    "loss.backward()  # add new gradients\n",
    "opt.step()  # change weights\n",
    "opt.zero_grad()  # clear gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dispose of old variables to avoid bugs later\n",
    "del x, y, y_predicted, loss, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:07:22.208909Z",
     "start_time": "2021-03-20T11:07:22.205867Z"
    }
   },
   "outputs": [],
   "source": [
    "# create network again just in case\n",
    "model = nn.Sequential()\n",
    "model.add_module(\"first\", nn.Linear(784, 1))\n",
    "model.add_module(\"second\", nn.Sigmoid())\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:07:22.792329Z",
     "start_time": "2021-03-20T11:07:22.783174Z"
    }
   },
   "outputs": [],
   "source": [
    "history = []\n",
    "\n",
    "for i in range(100):\n",
    "    # sample 256 random images\n",
    "    ix = np.random.randint(0, len(X_train), 256)\n",
    "    x_batch = torch.tensor(X_train[ix], dtype=torch.float32)\n",
    "    y_batch = torch.tensor(y_train[ix], dtype=torch.float32)\n",
    "\n",
    "    # predict probabilities\n",
    "    y_predicted = None  # YOUR CODE HERE\n",
    "\n",
    "    assert y_predicted.dim() == 1, \"did you forget to select first column with [:, 0]\"\n",
    "\n",
    "    # compute loss, just like before\n",
    "    loss = None # YOUR CODE HERE\n",
    "\n",
    "    # compute gradients\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Adam step\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # clear gradients\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    history.append(loss.data.numpy())\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(\"step #%i | mean loss = %.3f\" % (i, np.mean(history[-10:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Debugging tips:__\n",
    "* make sure your model predicts probabilities correctly. Just print them and see what's inside.\n",
    "* don't forget _minus_ sign in the loss function! It's a mistake 99% ppl do at some point.\n",
    "* make sure you zero-out gradients after each step. Srsly:)\n",
    "* In general, pytorch's error messages are quite helpful, read 'em before you google 'em.\n",
    "* if you see nan/inf, print what happens at each iteration to find our where exactly it occurs.\n",
    "  * If loss goes down and then turns nan midway through, try smaller learning rate. (Our current loss formula is unstable).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Let's see how our model performs on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use your model to predict classes (0 or 1) for all test samples\n",
    "# YOUR CODE HERE\n",
    "pred_test = np.array(pred_test > 0.5)\n",
    "\n",
    "assert isinstance(pred_test, np.ndarray), \"please return np array, not %s\" % type(pred_test)\n",
    "assert pred_test.shape == y_test.shape, \"please predict one class for each test sample\"\n",
    "assert np.in1d(pred_test, y_test).all(), \"please predict class indexes\"\n",
    "\n",
    "accuracy = np.mean(pred_test == y_test)\n",
    "print(\"Test accuracy: %.5f\" % accuracy)\n",
    "\n",
    "assert accuracy > 0.95, \"try training longer\"\n",
    "\n",
    "print(\"Great job!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More about pytorch:\n",
    "\n",
    "* Using torch on GPU and multi-GPU - [link](http://pytorch.org/docs/master/notes/cuda.html)\n",
    "* More tutorials on pytorch - [link](http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
    "* Pytorch examples - a repo that implements many cool DL models in pytorch - [link](https://github.com/pytorch/examples)\n",
    "* Practical pytorch - a repo that implements some... other cool DL models... yes, in pytorch - [link](https://github.com/spro/practical-pytorch)\n",
    "* And some more - [link](https://www.reddit.com/r/pytorch/comments/6z0yeo/pytorch_and_pytorch_tricks_for_kaggle/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "* PyTorch is a great choice to start writing simple (and not so simple) models from scratch\n",
    "* Autodiff will help you to get the derivatives even for complex functions (if implemented correctly), so use it wisely\n",
    "* Be careful with data normalization and monitor your loss/gradients values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
